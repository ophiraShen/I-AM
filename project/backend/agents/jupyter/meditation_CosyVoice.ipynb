{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 18:41:52,511 - modelscope - INFO - PyTorch version 2.6.0 Found.\n",
      "2025-04-20 18:41:52,513 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2025-04-20 18:41:52,550 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 2f5eeacd95c207bb2bc0f708cda0b4fd and a total number of 980 components indexed\n",
      "/root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to import ttsfrd, use WeTextProcessing instead\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/root/autodl-tmp/I-AM/CosyVoice\")\n",
    "sys.path.append(\"/root/autodl-tmp/I-AM/CosyVoice/third_party/Matcha-TTS\")\n",
    "\n",
    "import time\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import torchaudio, torch\n",
    "from cosyvoice.cli.cosyvoice import CosyVoice, CosyVoice2\n",
    "from cosyvoice.utils.file_utils import load_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosyVoice2TTS:\n",
    "    def __init__(self, model_path, prompts_config_path):\n",
    "        self.cosyvoice = CosyVoice2(\n",
    "            model_path,\n",
    "            load_jit=True,\n",
    "            load_onnx=False,\n",
    "            load_trt=False\n",
    "        )\n",
    "        with open(prompts_config_path, 'r', encoding='utf-8') as file:\n",
    "            self.prompts_config = yaml.safe_load(file)\n",
    "\n",
    "    def generate_audio(\n",
    "        self,\n",
    "        texts_path,\n",
    "        voice_type,\n",
    "        background_music_type,\n",
    "        output_path,\n",
    "        speed=1.0,\n",
    "        stream=False,\n",
    "        music_volume=0.2,\n",
    "        music_extension_duration=30,\n",
    "        fade_in_duration=3,\n",
    "        fade_out_duration=10,\n",
    "        max_retries=3\n",
    "    ):\n",
    "\n",
    "        with open(texts_path, 'r', encoding='utf-8') as file:\n",
    "            text_segments = yaml.safe_load(file)['sequences']\n",
    "\n",
    "        prompt_speech_16k = load_wav(self.prompts_config['prompts']['speech'][voice_type], 16000)\n",
    "        prompt_text = self.prompts_config['prompts']['text'][voice_type]\n",
    "\n",
    "        total_len = len(text_segments)\n",
    "\n",
    "        success = False\n",
    "        attempt = 0\n",
    "\n",
    "        while not success and attempt < max_retries:\n",
    "            attempt += 1\n",
    "            audio_segments = [None] * total_len\n",
    "            failed = False\n",
    "\n",
    "            print(f\"Attempt {attempt} of {max_retries}\")\n",
    "            # 创建进度条\n",
    "            pbar = tqdm(total=total_len, desc=\"Generating audio segments\")\n",
    "\n",
    "            for sentence in text_segments:\n",
    "                audio = self._generate_single(\n",
    "                    sentence['text'],\n",
    "                    prompt_text,\n",
    "                    prompt_speech_16k\n",
    "                )\n",
    "                if audio is None:\n",
    "                    print(f\"\\nFailed to generate audio for segment {idx}, retrying entire sequence...\")\n",
    "                    failed = True\n",
    "                    pbar.close()\n",
    "                    break\n",
    "\n",
    "                audio_segments[sentence['id']] = audio\n",
    "                pbar.update(1)\n",
    "            if not failed:\n",
    "                success = True\n",
    "                pbar.close()\n",
    "\n",
    "            if not success and attempt == max_retries:\n",
    "                raise Exception(\"Failed to generate all audio segments after maximum retries\")\n",
    "\n",
    "        print(\"Combining audio segments with background music...\")\n",
    "\n",
    "        combined_audio = self._combined_audios(\n",
    "            text_segments,\n",
    "            audio_segments,\n",
    "            background_music_type,\n",
    "            music_extension_duration,\n",
    "            fade_in_duration,\n",
    "            fade_out_duration\n",
    "        )\n",
    "\n",
    "        print(f\"Saving final audio to {output_path}\")\n",
    "\n",
    "        torchaudio.save(\n",
    "            output_path,\n",
    "            combined_audio,\n",
    "            self.cosyvoice.sample_rate\n",
    "        )\n",
    "\n",
    "        print(\"Audio generation completed!\")\n",
    "\n",
    "    def _generate_single(self, text, prompt_text, prompt_speech_16k, speed=1.0, stream=False):\n",
    "        try:\n",
    "            with torch.inference_mode(), torch.amp.autocast('cuda'):\n",
    "                for i, output in enumerate(self.cosyvoice.inference_zero_shot(\n",
    "                    text,\n",
    "                    prompt_text,\n",
    "                    prompt_speech_16k,\n",
    "                    speed=speed,\n",
    "                    stream=stream\n",
    "                )):\n",
    "                    audio = output['tts_speech']\n",
    "                return audio\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating audio for text: {text}\")\n",
    "            print(str(e))\n",
    "            return None\n",
    "\n",
    "    def _combined_audios(\n",
    "        self,\n",
    "        text_segments,\n",
    "        audio_segments,\n",
    "        background_music_type,\n",
    "        music_extension_duration=30,\n",
    "        fade_in_duration=3,\n",
    "        fade_out_duration=10\n",
    "    ):\n",
    "\n",
    "        combined_audio = self._generate_silence(fade_in_duration)\n",
    "        for i, audio in enumerate(audio_segments):\n",
    "            silence = self._generate_silence(text_segments[i]['duration'])\n",
    "            combined_audio = torch.cat([combined_audio, audio, silence], dim=1)\n",
    "        \n",
    "        music_extension_samples = self._generate_silence(music_extension_duration)\n",
    "        combined_audio =  torch.cat([combined_audio, music_extension_samples], dim=1)\n",
    "\n",
    "        background_music, bg_sample_rate = torchaudio.load(self.prompts_config['background_music'][background_music_type])\n",
    "\n",
    "        if background_music.shape[0] > 1:\n",
    "            background_music = torch.mean(background_music, dim=0, keepdim=True)\n",
    "\n",
    "        if bg_sample_rate != self.cosyvoice.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(bg_sample_rate, self.cosyvoice.sample_rate)\n",
    "            background_music = resampler(background_music)\n",
    "\n",
    "        # 调整背景音乐的长度以匹配语音长度\n",
    "        target_length = combined_audio.shape[1]\n",
    "        if background_music.shape[1] > target_length:\n",
    "            # 如果背景音乐更长，截取需要的部分\n",
    "            background_music = background_music[:, :target_length]\n",
    "        elif background_music.shape[1] < target_length:\n",
    "            # 如果背景音乐更短，循环播放直到达到所需长度\n",
    "            num_repeats = (target_length + background_music.shape[1] - 1) // background_music.shape[1]\n",
    "            background_music = background_music.repeat(1, num_repeats)\n",
    "            background_music = background_music[:, :target_length]\n",
    "\n",
    "        fade_in_samples = fade_in_duration * self.cosyvoice.sample_rate\n",
    "        fade_out_samples = fade_out_duration * self.cosyvoice.sample_rate\n",
    "\n",
    "        fade_in_curve = self._create_fade_curve(fade_in_samples, fade_in_samples, fade_in=True)\n",
    "        fade_out_curve = self._create_fade_curve(fade_out_samples, fade_out_samples, fade_in=False)\n",
    "\n",
    "        background_music[0, :fade_in_samples] *= fade_in_curve\n",
    "        background_music[0, -fade_out_samples:] *= fade_out_curve\n",
    "\n",
    "        # 调整背景音乐的音量（这里设置为语音的20%音量）\n",
    "        background_volume = 0.2\n",
    "        background_music = background_music * background_volume\n",
    "\n",
    "        # 混合语音和背景音乐\n",
    "        final_audio = combined_audio + background_music\n",
    "\n",
    "        # 防止音频溢出（可选）\n",
    "        if torch.max(torch.abs(final_audio)) > 1:\n",
    "            final_audio = final_audio / torch.max(torch.abs(final_audio))\n",
    "\n",
    "        return final_audio\n",
    "        \n",
    "\n",
    "    def _generate_silence(self, silence_duration):\n",
    "        return torch.zeros(1, silence_duration * self.cosyvoice.sample_rate)\n",
    "\n",
    "\n",
    "    def _create_fade_curve(self, length, fade_length, fade_in=True):\n",
    "        if fade_in:\n",
    "            return torch.linspace(0, 1, fade_length)\n",
    "        else:\n",
    "            return torch.linspace(1, 0, fade_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 18:43:20,220 INFO input frame rate=25\n",
      "\u001b[1;31m2025-04-20 18:43:21.356582103 [E:onnxruntime:Default, provider_bridge_ort.cc:1744 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "\u001b[m\n",
      "\u001b[0;93m2025-04-20 18:43:21.356604054 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:870 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.\u001b[m\n",
      "2025-04-20 18:43:21,920 WETEXT INFO found existing fst: /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/zh_tn_tagger.fst\n",
      "2025-04-20 18:43:21,920 WETEXT INFO found existing fst: /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/zh_tn_tagger.fst\n",
      "2025-04-20 18:43:21,920 INFO found existing fst: /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/zh_tn_tagger.fst\n",
      "2025-04-20 18:43:21,922 WETEXT INFO                     /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/zh_tn_verbalizer.fst\n",
      "2025-04-20 18:43:21,922 WETEXT INFO                     /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/zh_tn_verbalizer.fst\n",
      "2025-04-20 18:43:21,922 INFO                     /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/zh_tn_verbalizer.fst\n",
      "2025-04-20 18:43:21,923 WETEXT INFO skip building fst for zh_normalizer ...\n",
      "2025-04-20 18:43:21,923 WETEXT INFO skip building fst for zh_normalizer ...\n",
      "2025-04-20 18:43:21,923 INFO skip building fst for zh_normalizer ...\n",
      "2025-04-20 18:43:22,369 WETEXT INFO found existing fst: /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/en_tn_tagger.fst\n",
      "2025-04-20 18:43:22,369 WETEXT INFO found existing fst: /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/en_tn_tagger.fst\n",
      "2025-04-20 18:43:22,369 INFO found existing fst: /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/en_tn_tagger.fst\n",
      "2025-04-20 18:43:22,370 WETEXT INFO                     /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/en_tn_verbalizer.fst\n",
      "2025-04-20 18:43:22,370 WETEXT INFO                     /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/en_tn_verbalizer.fst\n",
      "2025-04-20 18:43:22,370 INFO                     /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/en_tn_verbalizer.fst\n",
      "2025-04-20 18:43:22,371 WETEXT INFO skip building fst for en_normalizer ...\n",
      "2025-04-20 18:43:22,371 WETEXT INFO skip building fst for en_normalizer ...\n",
      "2025-04-20 18:43:22,371 INFO skip building fst for en_normalizer ...\n"
     ]
    }
   ],
   "source": [
    "tts = CosyVoice2TTS(\n",
    "    model_path='/root/autodl-fs/cosyvoice/pretrained_models/CosyVoice2-0.5B',\n",
    "    prompts_config_path='/root/autodl-tmp/I-AM/project/backend/config/meditation.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tts.generate_audio(\n",
    "    texts_path=\"/root/autodl-tmp/I-AM/project/backend/agents/jupyter/output/script/exam.yaml\",\n",
    "    voice_type=\"male1\",\n",
    "    background_music_type=\"bmusic_01\",\n",
    "    output_path=\"/root/autodl-tmp/I-AM/project/backend/agents/jupyter/output/temp/exam.wav\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拼接音频文件\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from pydub import AudioSegment\n",
    "\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    \"\"\"用于实现自然排序的键函数\"\"\"\n",
    "    result = []\n",
    "    # 使用正则表达式分割字符串，保留数字\n",
    "    parts = re.split('([0-9]+)', s)\n",
    "    \n",
    "    for text in parts:\n",
    "        if text.isdigit():\n",
    "            # 如果是数字，转换为整数\n",
    "            result.append(int(text))\n",
    "        else:\n",
    "            # 如果是文本，转换为小写\n",
    "            result.append(text.lower())\n",
    "    \n",
    "    return result\n",
    "\n",
    "def combine_wav_files_with_pause_and_music(base_path, output_path, pause_durations, background_music_path, music_extension_duration=15000, crossfade_duration=100, fade_out_duration=5000):\n",
    "    \"\"\"\n",
    "    连接多个 WAV 文件，并在之间添加停顿，同时添加背景音乐\n",
    "    \n",
    "    参数:\n",
    "    base_path: 输入文件的基础路径\n",
    "    output_path: 输出文件路径\n",
    "    pause_durations: 每个段落之间的停顿时长列表(毫秒)\n",
    "    background_music_path: 背景音乐文件路径\n",
    "    music_extension_duration: 背景音乐在主音频结束后继续播放的时长(毫秒)\n",
    "    crossfade_duration: 交叉淡入淡出的持续时间(毫秒)\n",
    "    fade_out_duration: 背景音乐的渐出时长(毫秒)\n",
    "    \"\"\"\n",
    "    wav_files = glob.glob(os.path.join(base_path, \"*.wav\"))\n",
    "    wav_files.sort(key=natural_sort_key)\n",
    "\n",
    "    for wav_file in wav_files:\n",
    "        if not os.path.exists(wav_file):\n",
    "            print(f\"文件 {wav_file} 不存在\")\n",
    "            return\n",
    "    \n",
    "    print(f\"找到以下文件: \")\n",
    "    for wav_file in wav_files:\n",
    "        print(wav_file)\n",
    "    \n",
    "    # 加载5秒静音\n",
    "    combined = AudioSegment.silent(duration=5000)\n",
    "    \n",
    "    # 依次添加其他音频文件\n",
    "    for i, wav_file in enumerate(wav_files):\n",
    "        # 创建静音片段\n",
    "        pause = AudioSegment.silent(duration=pause_durations[i])\n",
    "        \n",
    "        # 加载并添加下一个音频文件\n",
    "        next_segment = AudioSegment.from_wav(wav_file)\n",
    "        combined = combined.append(next_segment, crossfade=crossfade_duration)\n",
    "\n",
    "        # 添加停顿\n",
    "        combined = combined + pause\n",
    "    \n",
    "    # 添加额外的静音以延长背景音乐\n",
    "    combined = combined + AudioSegment.silent(duration=music_extension_duration)\n",
    "    \n",
    "    # 加载背景音乐\n",
    "    background_music = AudioSegment.from_file(background_music_path)\n",
    "    \n",
    "    # 调整背景音乐音量\n",
    "    background_music = background_music - 15  # 减少音量，单位为dB\n",
    "    \n",
    "    # 截断或循环背景音乐以匹配合并音频的长度\n",
    "    if len(background_music) < len(combined):\n",
    "        background_music = background_music * ((len(combined) // len(background_music)) + 1)\n",
    "    background_music = background_music[:len(combined)]\n",
    "    \n",
    "    # 添加渐入和渐出效果\n",
    "    background_music = background_music.fade_in(fade_in_duration).fade_out(fade_out_duration)\n",
    "    \n",
    "    # 叠加背景音乐\n",
    "    combined_with_music = combined.overlay(background_music)\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # 导出合并后的文件\n",
    "    combined_with_music.export(output_path, format=\"wav\")\n",
    "    print(f\"已合并所有音频并添加背景音乐到：{output_path}\")\n",
    "\n",
    "# 使用示例\n",
    "base_path = \"output/tts/sleep/female2\"\n",
    "output_path = \"output/tts/sleep/female2_combined_with_bmusic_03.wav\"\n",
    "pause_durations = love_texts_silence_durations\n",
    "background_music_path = \"/root/autodl-tmp/I-AM/project/backend/agents/prompts/background_music/bmusic_03.wav\"\n",
    "music_extension_duration = 30000  # 背景音乐延长30秒\n",
    "fade_in_duration = 3000\n",
    "fade_out_duration = 10000  # 渐出时长10秒\n",
    "\n",
    "combine_wav_files_with_pause_and_music(\n",
    "    base_path,\n",
    "    output_path,\n",
    "    pause_durations,\n",
    "    background_music_path,\n",
    "    music_extension_duration,\n",
    "    fade_out_duration=fade_out_duration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manifest_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
