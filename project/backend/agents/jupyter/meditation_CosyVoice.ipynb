{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-12 21:55:07,643 - modelscope - INFO - PyTorch version 2.5.1 Found.\n",
      "2025-01-12 21:55:07,645 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2025-01-12 21:55:07,689 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 2f5eeacd95c207bb2bc0f708cda0b4fd and a total number of 980 components indexed\n",
      "/root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to import ttsfrd, use WeTextProcessing instead\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/root/autodl-tmp/I-AM/CosyVoice\")\n",
    "sys.path.append(\"/root/autodl-tmp/I-AM/CosyVoice/third_party/Matcha-TTS\")\n",
    "\n",
    "import time\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import torchaudio, torch\n",
    "from cosyvoice.cli.cosyvoice import CosyVoice, CosyVoice2\n",
    "from cosyvoice.utils.file_utils import load_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosyVoice2TTS:\n",
    "    def __init__(self, model_path, prompts_config_path):\n",
    "        self.cosyvoice = CosyVoice2(\n",
    "            model_path,\n",
    "            load_jit=True,\n",
    "            load_onnx=False,\n",
    "            load_trt=False\n",
    "        )\n",
    "        with open(prompts_config_path, 'r', encoding='utf-8') as file:\n",
    "            self.prompts_config = yaml.safe_load(file)\n",
    "\n",
    "    def generate_audio(\n",
    "        self,\n",
    "        texts_path,\n",
    "        voice_type,\n",
    "        background_music_type,\n",
    "        output_path,\n",
    "        speed=1.0,\n",
    "        stream=False,\n",
    "        music_volume=0.2,\n",
    "        music_extension_duration=30,\n",
    "        fade_in_duration=3,\n",
    "        fade_out_duration=10,\n",
    "        max_retries=3\n",
    "    ):\n",
    "\n",
    "        with open(texts_path, 'r', encoding='utf-8') as file:\n",
    "            text_segments = yaml.safe_load(file)['sequences']\n",
    "\n",
    "        prompt_speech_16k = load_wav(self.prompts_config['prompts']['speech'][voice_type], 16000)\n",
    "        prompt_text = self.prompts_config['prompts']['text'][voice_type]\n",
    "\n",
    "        total_len = len(text_segments)\n",
    "\n",
    "        success = False\n",
    "        attempt = 0\n",
    "\n",
    "        while not success and attempt < max_retries:\n",
    "            attempt += 1\n",
    "            audio_segments = [None] * total_len\n",
    "            failed = False\n",
    "\n",
    "            print(f\"Attempt {attempt} of {max_retries}\")\n",
    "            # 创建进度条\n",
    "            pbar = tqdm(total=total_len, desc=\"Generating audio segments\")\n",
    "\n",
    "            for sentence in text_segments:\n",
    "                audio = self._generate_single(\n",
    "                    sentence['text'],\n",
    "                    prompt_text,\n",
    "                    prompt_speech_16k\n",
    "                )\n",
    "                if audio is None:\n",
    "                    print(f\"\\nFailed to generate audio for segment {idx}, retrying entire sequence...\")\n",
    "                    failed = True\n",
    "                    pbar.close()\n",
    "                    break\n",
    "\n",
    "                audio_segments[sentence['id']] = audio\n",
    "                pbar.update(1)\n",
    "            if not failed:\n",
    "                success = True\n",
    "                pbar.close()\n",
    "\n",
    "            if not success and attempt == max_retries:\n",
    "                raise Exception(\"Failed to generate all audio segments after maximum retries\")\n",
    "\n",
    "        print(\"Combining audio segments with background music...\")\n",
    "\n",
    "        combined_audio = self._combined_audios(\n",
    "            text_segments,\n",
    "            audio_segments,\n",
    "            background_music_type,\n",
    "            music_extension_duration,\n",
    "            fade_in_duration,\n",
    "            fade_out_duration\n",
    "        )\n",
    "\n",
    "        print(f\"Saving final audio to {output_path}\")\n",
    "\n",
    "        torchaudio.save(\n",
    "            output_path,\n",
    "            combined_audio,\n",
    "            self.cosyvoice.sample_rate\n",
    "        )\n",
    "\n",
    "        print(\"Audio generation completed!\")\n",
    "\n",
    "    def _generate_single(self, text, prompt_text, prompt_speech_16k, speed=1.0, stream=False):\n",
    "        try:\n",
    "            with torch.inference_mode(), torch.amp.autocast('cuda'):\n",
    "                for i, output in enumerate(self.cosyvoice.inference_zero_shot(\n",
    "                    text,\n",
    "                    prompt_text,\n",
    "                    prompt_speech_16k,\n",
    "                    speed=speed,\n",
    "                    stream=stream\n",
    "                )):\n",
    "                    audio = output['tts_speech']\n",
    "                return audio\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating audio for text: {text}\")\n",
    "            print(str(e))\n",
    "            return None\n",
    "\n",
    "    def _combined_audios(\n",
    "        self,\n",
    "        text_segments,\n",
    "        audio_segments,\n",
    "        background_music_type,\n",
    "        music_extension_duration=30,\n",
    "        fade_in_duration=3,\n",
    "        fade_out_duration=10\n",
    "    ):\n",
    "\n",
    "        combined_audio = self._generate_silence(fade_in_duration)\n",
    "        for i, audio in enumerate(audio_segments):\n",
    "            silence = self._generate_silence(text_segments[i]['duration'])\n",
    "            combined_audio = torch.cat([combined_audio, audio, silence], dim=1)\n",
    "        \n",
    "        music_extension_samples = self._generate_silence(music_extension_duration)\n",
    "        combined_audio =  torch.cat([combined_audio, music_extension_samples], dim=1)\n",
    "\n",
    "        background_music, bg_sample_rate = torchaudio.load(self.prompts_config['background_music'][background_music_type])\n",
    "\n",
    "        if background_music.shape[0] > 1:\n",
    "            background_music = torch.mean(background_music, dim=0, keepdim=True)\n",
    "\n",
    "        if bg_sample_rate != self.cosyvoice.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(bg_sample_rate, self.cosyvoice.sample_rate)\n",
    "            background_music = resampler(background_music)\n",
    "\n",
    "        # 调整背景音乐的长度以匹配语音长度\n",
    "        target_length = combined_audio.shape[1]\n",
    "        if background_music.shape[1] > target_length:\n",
    "            # 如果背景音乐更长，截取需要的部分\n",
    "            background_music = background_music[:, :target_length]\n",
    "        elif background_music.shape[1] < target_length:\n",
    "            # 如果背景音乐更短，循环播放直到达到所需长度\n",
    "            num_repeats = (target_length + background_music.shape[1] - 1) // background_music.shape[1]\n",
    "            background_music = background_music.repeat(1, num_repeats)\n",
    "            background_music = background_music[:, :target_length]\n",
    "\n",
    "        fade_in_samples = fade_in_duration * self.cosyvoice.sample_rate\n",
    "        fade_out_samples = fade_out_duration * self.cosyvoice.sample_rate\n",
    "\n",
    "        fade_in_curve = self._create_fade_curve(fade_in_samples, fade_in_samples, fade_in=True)\n",
    "        fade_out_curve = self._create_fade_curve(fade_out_samples, fade_out_samples, fade_in=False)\n",
    "\n",
    "        background_music[0, :fade_in_samples] *= fade_in_curve\n",
    "        background_music[0, -fade_out_samples:] *= fade_out_curve\n",
    "\n",
    "        # 调整背景音乐的音量（这里设置为语音的20%音量）\n",
    "        background_volume = 0.2\n",
    "        background_music = background_music * background_volume\n",
    "\n",
    "        # 混合语音和背景音乐\n",
    "        final_audio = combined_audio + background_music\n",
    "\n",
    "        # 防止音频溢出（可选）\n",
    "        if torch.max(torch.abs(final_audio)) > 1:\n",
    "            final_audio = final_audio / torch.max(torch.abs(final_audio))\n",
    "\n",
    "        return final_audio\n",
    "        \n",
    "\n",
    "    def _generate_silence(self, silence_duration):\n",
    "        return torch.zeros(1, silence_duration * self.cosyvoice.sample_rate)\n",
    "\n",
    "\n",
    "    def _create_fade_curve(self, length, fade_length, fade_in=True):\n",
    "        if fade_in:\n",
    "            return torch.linspace(0, 1, fade_length)\n",
    "        else:\n",
    "            return torch.linspace(1, 0, fade_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\n",
      "  deprecate(\"LoRACompatibleLinear\", \"1.0.0\", deprecation_message)\n",
      "2025-01-12 21:55:33,516 INFO input frame rate=25\n",
      "/root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/root/autodl-tmp/I-AM/CosyVoice/cosyvoice/dataset/processor.py:24: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend('soundfile')\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[1;31m2025-01-12 21:55:38.080601858 [E:onnxruntime:Default, provider_bridge_ort.cc:1744 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "\u001b[m\n",
      "\u001b[0;93m2025-01-12 21:55:38.080624335 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:870 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.\u001b[m\n",
      "2025-01-12 21:55:38,917 WETEXT INFO found existing fst: /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/zh_tn_tagger.fst\n",
      "2025-01-12 21:55:38,917 INFO found existing fst: /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/zh_tn_tagger.fst\n",
      "2025-01-12 21:55:38,919 WETEXT INFO                     /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/zh_tn_verbalizer.fst\n",
      "2025-01-12 21:55:38,919 INFO                     /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/zh_tn_verbalizer.fst\n",
      "2025-01-12 21:55:38,920 WETEXT INFO skip building fst for zh_normalizer ...\n",
      "2025-01-12 21:55:38,920 INFO skip building fst for zh_normalizer ...\n",
      "2025-01-12 21:55:39,483 WETEXT INFO found existing fst: /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/en_tn_tagger.fst\n",
      "2025-01-12 21:55:39,483 INFO found existing fst: /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/en_tn_tagger.fst\n",
      "2025-01-12 21:55:39,486 WETEXT INFO                     /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/en_tn_verbalizer.fst\n",
      "2025-01-12 21:55:39,486 INFO                     /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/en_tn_verbalizer.fst\n",
      "2025-01-12 21:55:39,488 WETEXT INFO skip building fst for en_normalizer ...\n",
      "2025-01-12 21:55:39,488 INFO skip building fst for en_normalizer ...\n",
      "/root/autodl-tmp/I-AM/CosyVoice/cosyvoice/cli/model.py:291: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.llm.load_state_dict(torch.load(llm_model, map_location=self.device), strict=True)\n",
      "/root/autodl-tmp/I-AM/CosyVoice/cosyvoice/cli/model.py:293: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.flow.load_state_dict(torch.load(flow_model, map_location=self.device), strict=True)\n",
      "/root/autodl-tmp/I-AM/CosyVoice/cosyvoice/cli/model.py:297: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hift_state_dict = {k.replace('generator.', ''): v for k, v in torch.load(hift_model, map_location=self.device).items()}\n"
     ]
    }
   ],
   "source": [
    "tts = CosyVoice2TTS(\n",
    "    model_path='/root/autodl-fs/cosyvoice/pretrained_models/CosyVoice2-0.5B',\n",
    "    prompts_config_path='/root/autodl-tmp/I-AM/project/backend/agents/prompts/meditation/tts/zero_shot.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to open the input \"/root/autodl-tmp/I-AM/project/backend/agents/prompts/vocals/male1_[12sec].wav\" (No such file or directory).\nException raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f48bcf6c446 in /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f48bcf166e4 in /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/torch/lib/libc10.so)\nframe #2: <unknown function> + 0x42134 (0x7f485d017134 in /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x7f485d019b34 in /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #4: <unknown function> + 0x3a8de (0x7f47f2fea8de in /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/torio/lib/_torio_ffmpeg4.so)\nframe #5: <unknown function> + 0x323ee (0x7f47f2fe23ee in /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/torio/lib/_torio_ffmpeg4.so)\nframe #6: /root/miniconda3/envs/manifest_app/bin/python() [0x528b17]\nframe #7: _PyObject_MakeTpCall + 0x27c (0x50452c in /root/miniconda3/envs/manifest_app/bin/python)\nframe #8: /root/miniconda3/envs/manifest_app/bin/python() [0x557ac9]\nframe #9: /root/miniconda3/envs/manifest_app/bin/python() [0x540849]\nframe #10: /root/miniconda3/envs/manifest_app/bin/python() [0x50492c]\nframe #11: <unknown function> + 0xfc8b (0x7f48bca92c8b in /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/torchaudio/lib/_torchaudio.so)\nframe #12: _PyObject_MakeTpCall + 0x27c (0x50452c in /root/miniconda3/envs/manifest_app/bin/python)\nframe #13: _PyEval_EvalFrameDefault + 0x6a6 (0x511a76 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #14: _PyFunction_Vectorcall + 0x173 (0x539153 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #15: /root/miniconda3/envs/manifest_app/bin/python() [0x5404d2]\nframe #16: _PyObject_MakeTpCall + 0x243 (0x5044f3 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #17: _PyEval_EvalFrameDefault + 0x6a6 (0x511a76 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #18: /root/miniconda3/envs/manifest_app/bin/python() [0x5cc3aa]\nframe #19: PyEval_EvalCode + 0x9f (0x5cba7f in /root/miniconda3/envs/manifest_app/bin/python)\nframe #20: /root/miniconda3/envs/manifest_app/bin/python() [0x5e5283]\nframe #21: _PyEval_EvalFrameDefault + 0x3593 (0x514963 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #22: /root/miniconda3/envs/manifest_app/bin/python() [0x5e0c8a]\nframe #23: _PyEval_EvalFrameDefault + 0x31eb (0x5145bb in /root/miniconda3/envs/manifest_app/bin/python)\nframe #24: /root/miniconda3/envs/manifest_app/bin/python() [0x5e0c8a]\nframe #25: _PyEval_EvalFrameDefault + 0x31eb (0x5145bb in /root/miniconda3/envs/manifest_app/bin/python)\nframe #26: /root/miniconda3/envs/manifest_app/bin/python() [0x5e0c8a]\nframe #27: /root/miniconda3/envs/manifest_app/bin/python() [0x5e32f6]\nframe #28: _PyEval_EvalFrameDefault + 0x38b6 (0x514c86 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #29: /root/miniconda3/envs/manifest_app/bin/python() [0x5581df]\nframe #30: /root/miniconda3/envs/manifest_app/bin/python() [0x5579ce]\nframe #31: PyObject_Call + 0x12c (0x5430ac in /root/miniconda3/envs/manifest_app/bin/python)\nframe #32: _PyEval_EvalFrameDefault + 0x47c0 (0x515b90 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #33: /root/miniconda3/envs/manifest_app/bin/python() [0x5e0c8a]\nframe #34: _PyEval_EvalFrameDefault + 0x31eb (0x5145bb in /root/miniconda3/envs/manifest_app/bin/python)\nframe #35: /root/miniconda3/envs/manifest_app/bin/python() [0x5e0c8a]\nframe #36: _PyEval_EvalFrameDefault + 0x31eb (0x5145bb in /root/miniconda3/envs/manifest_app/bin/python)\nframe #37: /root/miniconda3/envs/manifest_app/bin/python() [0x5e0c8a]\nframe #38: _PyEval_EvalFrameDefault + 0x31eb (0x5145bb in /root/miniconda3/envs/manifest_app/bin/python)\nframe #39: /root/miniconda3/envs/manifest_app/bin/python() [0x5e0c8a]\nframe #40: _PyEval_EvalFrameDefault + 0x31eb (0x5145bb in /root/miniconda3/envs/manifest_app/bin/python)\nframe #41: /root/miniconda3/envs/manifest_app/bin/python() [0x5e0c8a]\nframe #42: _PyEval_EvalFrameDefault + 0x31eb (0x5145bb in /root/miniconda3/envs/manifest_app/bin/python)\nframe #43: /root/miniconda3/envs/manifest_app/bin/python() [0x5e0c8a]\nframe #44: <unknown function> + 0x79fb (0x7f48d15a89fb in /root/miniconda3/envs/manifest_app/lib/python3.11/lib-dynload/_asyncio.cpython-311-x86_64-linux-gnu.so)\nframe #45: /root/miniconda3/envs/manifest_app/bin/python() [0x526cfb]\nframe #46: /root/miniconda3/envs/manifest_app/bin/python() [0x4c74f0]\nframe #47: /root/miniconda3/envs/manifest_app/bin/python() [0x4cc53e]\nframe #48: /root/miniconda3/envs/manifest_app/bin/python() [0x51eb47]\nframe #49: _PyEval_EvalFrameDefault + 0x8e37 (0x51a207 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #50: /root/miniconda3/envs/manifest_app/bin/python() [0x5cc3aa]\nframe #51: PyEval_EvalCode + 0x9f (0x5cba7f in /root/miniconda3/envs/manifest_app/bin/python)\nframe #52: /root/miniconda3/envs/manifest_app/bin/python() [0x5e5283]\nframe #53: /root/miniconda3/envs/manifest_app/bin/python() [0x51eb47]\nframe #54: PyObject_Vectorcall + 0x31 (0x51ea31 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #55: _PyEval_EvalFrameDefault + 0x6a6 (0x511a76 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #56: _PyFunction_Vectorcall + 0x173 (0x539153 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #57: /root/miniconda3/envs/manifest_app/bin/python() [0x5f784f]\nframe #58: Py_RunMain + 0x14a (0x5f725a in /root/miniconda3/envs/manifest_app/bin/python)\nframe #59: Py_BytesMain + 0x39 (0x5bc149 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #60: <unknown function> + 0x29d90 (0x7f48d19b0d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #61: __libc_start_main + 0x80 (0x7f48d19b0e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #62: /root/miniconda3/envs/manifest_app/bin/python() [0x5bbf93]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_audio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/root/autodl-tmp/I-AM/project/backend/agents/jupyter/output/script/exam.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvoice_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmale1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackground_music_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbmusic_01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/root/autodl-tmp/I-AM/project/backend/agents/jupyter/output/temp/exam.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m, in \u001b[0;36mCosyVoice2TTS.generate_audio\u001b[0;34m(self, texts_path, voice_type, background_music_type, output_path, speed, stream, music_volume, music_extension_duration, fade_in_duration, fade_out_duration, max_retries)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(texts_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     28\u001b[0m     text_segments \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(file)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequences\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m prompt_speech_16k \u001b[38;5;241m=\u001b[39m \u001b[43mload_wav\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompts_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspeech\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvoice_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m prompt_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompts\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][voice_type]\n\u001b[1;32m     33\u001b[0m total_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(text_segments)\n",
      "File \u001b[0;32m~/autodl-tmp/I-AM/CosyVoice/cosyvoice/utils/file_utils.py:42\u001b[0m, in \u001b[0;36mload_wav\u001b[0;34m(wav, target_sr)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_wav\u001b[39m(wav, target_sr):\n\u001b[0;32m---> 42\u001b[0m     speech, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     speech \u001b[38;5;241m=\u001b[39m speech\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_rate \u001b[38;5;241m!=\u001b[39m target_sr:\n",
      "File \u001b[0;32m~/miniconda3/envs/manifest_app/lib/python3.11/site-packages/torchaudio/_backend/utils.py:205\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m        `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m backend \u001b[38;5;241m=\u001b[39m dispatcher(uri, \u001b[38;5;28mformat\u001b[39m, backend)\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/manifest_app/lib/python3.11/site-packages/torchaudio/_backend/ffmpeg.py:297\u001b[0m, in \u001b[0;36mFFmpegBackend.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    289\u001b[0m     uri: InputType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    295\u001b[0m     buffer_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m,\n\u001b[1;32m    296\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/manifest_app/lib/python3.11/site-packages/torchaudio/_backend/ffmpeg.py:88\u001b[0m, in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(src, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvorbis\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mogg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 88\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStreamReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m sample_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(s\u001b[38;5;241m.\u001b[39mget_src_stream_info(s\u001b[38;5;241m.\u001b[39mdefault_audio_stream)\u001b[38;5;241m.\u001b[39msample_rate)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;241m=\u001b[39m _get_load_filter(frame_offset, num_frames, convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/manifest_app/lib/python3.11/site-packages/torio/io/_streaming_media_decoder.py:526\u001b[0m, in \u001b[0;36mStreamingMediaDecoder.__init__\u001b[0;34m(self, src, format, option, buffer_size)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_be \u001b[38;5;241m=\u001b[39m ffmpeg_ext\u001b[38;5;241m.\u001b[39mStreamingMediaDecoderFileObj(src, \u001b[38;5;28mformat\u001b[39m, option, buffer_size)\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_be \u001b[38;5;241m=\u001b[39m \u001b[43mffmpeg_ext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStreamingMediaDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moption\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_be\u001b[38;5;241m.\u001b[39mfind_best_audio_stream()\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_audio_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m i\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to open the input \"/root/autodl-tmp/I-AM/project/backend/agents/prompts/vocals/male1_[12sec].wav\" (No such file or directory).\nException raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f48bcf6c446 in /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f48bcf166e4 in /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/torch/lib/libc10.so)\nframe #2: <unknown function> + 0x42134 (0x7f485d017134 in /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x7f485d019b34 in /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #4: <unknown function> + 0x3a8de (0x7f47f2fea8de in /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/torio/lib/_torio_ffmpeg4.so)\nframe #5: <unknown function> + 0x323ee (0x7f47f2fe23ee in /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/torio/lib/_torio_ffmpeg4.so)\nframe #6: /root/miniconda3/envs/manifest_app/bin/python() [0x528b17]\nframe #7: _PyObject_MakeTpCall + 0x27c (0x50452c in /root/miniconda3/envs/manifest_app/bin/python)\nframe #8: /root/miniconda3/envs/manifest_app/bin/python() [0x557ac9]\nframe #9: /root/miniconda3/envs/manifest_app/bin/python() [0x540849]\nframe #10: /root/miniconda3/envs/manifest_app/bin/python() [0x50492c]\nframe #11: <unknown function> + 0xfc8b (0x7f48bca92c8b in /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/torchaudio/lib/_torchaudio.so)\nframe #12: _PyObject_MakeTpCall + 0x27c (0x50452c in /root/miniconda3/envs/manifest_app/bin/python)\nframe #13: _PyEval_EvalFrameDefault + 0x6a6 (0x511a76 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #14: _PyFunction_Vectorcall + 0x173 (0x539153 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #15: /root/miniconda3/envs/manifest_app/bin/python() [0x5404d2]\nframe #16: _PyObject_MakeTpCall + 0x243 (0x5044f3 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #17: _PyEval_EvalFrameDefault + 0x6a6 (0x511a76 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #18: /root/miniconda3/envs/manifest_app/bin/python() [0x5cc3aa]\nframe #19: PyEval_EvalCode + 0x9f (0x5cba7f in /root/miniconda3/envs/manifest_app/bin/python)\nframe #20: /root/miniconda3/envs/manifest_app/bin/python() [0x5e5283]\nframe #21: _PyEval_EvalFrameDefault + 0x3593 (0x514963 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #22: /root/miniconda3/envs/manifest_app/bin/python() [0x5e0c8a]\nframe #23: _PyEval_EvalFrameDefault + 0x31eb (0x5145bb in /root/miniconda3/envs/manifest_app/bin/python)\nframe #24: /root/miniconda3/envs/manifest_app/bin/python() [0x5e0c8a]\nframe #25: _PyEval_EvalFrameDefault + 0x31eb (0x5145bb in /root/miniconda3/envs/manifest_app/bin/python)\nframe #26: /root/miniconda3/envs/manifest_app/bin/python() [0x5e0c8a]\nframe #27: /root/miniconda3/envs/manifest_app/bin/python() [0x5e32f6]\nframe #28: _PyEval_EvalFrameDefault + 0x38b6 (0x514c86 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #29: /root/miniconda3/envs/manifest_app/bin/python() [0x5581df]\nframe #30: /root/miniconda3/envs/manifest_app/bin/python() [0x5579ce]\nframe #31: PyObject_Call + 0x12c (0x5430ac in /root/miniconda3/envs/manifest_app/bin/python)\nframe #32: _PyEval_EvalFrameDefault + 0x47c0 (0x515b90 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #33: /root/miniconda3/envs/manifest_app/bin/python() [0x5e0c8a]\nframe #34: _PyEval_EvalFrameDefault + 0x31eb (0x5145bb in /root/miniconda3/envs/manifest_app/bin/python)\nframe #35: /root/miniconda3/envs/manifest_app/bin/python() [0x5e0c8a]\nframe #36: _PyEval_EvalFrameDefault + 0x31eb (0x5145bb in /root/miniconda3/envs/manifest_app/bin/python)\nframe #37: /root/miniconda3/envs/manifest_app/bin/python() [0x5e0c8a]\nframe #38: _PyEval_EvalFrameDefault + 0x31eb (0x5145bb in /root/miniconda3/envs/manifest_app/bin/python)\nframe #39: /root/miniconda3/envs/manifest_app/bin/python() [0x5e0c8a]\nframe #40: _PyEval_EvalFrameDefault + 0x31eb (0x5145bb in /root/miniconda3/envs/manifest_app/bin/python)\nframe #41: /root/miniconda3/envs/manifest_app/bin/python() [0x5e0c8a]\nframe #42: _PyEval_EvalFrameDefault + 0x31eb (0x5145bb in /root/miniconda3/envs/manifest_app/bin/python)\nframe #43: /root/miniconda3/envs/manifest_app/bin/python() [0x5e0c8a]\nframe #44: <unknown function> + 0x79fb (0x7f48d15a89fb in /root/miniconda3/envs/manifest_app/lib/python3.11/lib-dynload/_asyncio.cpython-311-x86_64-linux-gnu.so)\nframe #45: /root/miniconda3/envs/manifest_app/bin/python() [0x526cfb]\nframe #46: /root/miniconda3/envs/manifest_app/bin/python() [0x4c74f0]\nframe #47: /root/miniconda3/envs/manifest_app/bin/python() [0x4cc53e]\nframe #48: /root/miniconda3/envs/manifest_app/bin/python() [0x51eb47]\nframe #49: _PyEval_EvalFrameDefault + 0x8e37 (0x51a207 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #50: /root/miniconda3/envs/manifest_app/bin/python() [0x5cc3aa]\nframe #51: PyEval_EvalCode + 0x9f (0x5cba7f in /root/miniconda3/envs/manifest_app/bin/python)\nframe #52: /root/miniconda3/envs/manifest_app/bin/python() [0x5e5283]\nframe #53: /root/miniconda3/envs/manifest_app/bin/python() [0x51eb47]\nframe #54: PyObject_Vectorcall + 0x31 (0x51ea31 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #55: _PyEval_EvalFrameDefault + 0x6a6 (0x511a76 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #56: _PyFunction_Vectorcall + 0x173 (0x539153 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #57: /root/miniconda3/envs/manifest_app/bin/python() [0x5f784f]\nframe #58: Py_RunMain + 0x14a (0x5f725a in /root/miniconda3/envs/manifest_app/bin/python)\nframe #59: Py_BytesMain + 0x39 (0x5bc149 in /root/miniconda3/envs/manifest_app/bin/python)\nframe #60: <unknown function> + 0x29d90 (0x7f48d19b0d90 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #61: __libc_start_main + 0x80 (0x7f48d19b0e40 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #62: /root/miniconda3/envs/manifest_app/bin/python() [0x5bbf93]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tts.generate_audio(\n",
    "    texts_path=\"/root/autodl-tmp/I-AM/project/backend/agents/jupyter/output/script/exam.yaml\",\n",
    "    voice_type=\"male1\",\n",
    "    background_music_type=\"bmusic_01\",\n",
    "    output_path=\"/root/autodl-tmp/I-AM/project/backend/agents/jupyter/output/temp/exam.wav\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拼接音频文件\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from pydub import AudioSegment\n",
    "\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    \"\"\"用于实现自然排序的键函数\"\"\"\n",
    "    result = []\n",
    "    # 使用正则表达式分割字符串，保留数字\n",
    "    parts = re.split('([0-9]+)', s)\n",
    "    \n",
    "    for text in parts:\n",
    "        if text.isdigit():\n",
    "            # 如果是数字，转换为整数\n",
    "            result.append(int(text))\n",
    "        else:\n",
    "            # 如果是文本，转换为小写\n",
    "            result.append(text.lower())\n",
    "    \n",
    "    return result\n",
    "\n",
    "def combine_wav_files_with_pause_and_music(base_path, output_path, pause_durations, background_music_path, music_extension_duration=15000, crossfade_duration=100, fade_out_duration=5000):\n",
    "    \"\"\"\n",
    "    连接多个 WAV 文件，并在之间添加停顿，同时添加背景音乐\n",
    "    \n",
    "    参数:\n",
    "    base_path: 输入文件的基础路径\n",
    "    output_path: 输出文件路径\n",
    "    pause_durations: 每个段落之间的停顿时长列表(毫秒)\n",
    "    background_music_path: 背景音乐文件路径\n",
    "    music_extension_duration: 背景音乐在主音频结束后继续播放的时长(毫秒)\n",
    "    crossfade_duration: 交叉淡入淡出的持续时间(毫秒)\n",
    "    fade_out_duration: 背景音乐的渐出时长(毫秒)\n",
    "    \"\"\"\n",
    "    wav_files = glob.glob(os.path.join(base_path, \"*.wav\"))\n",
    "    wav_files.sort(key=natural_sort_key)\n",
    "\n",
    "    for wav_file in wav_files:\n",
    "        if not os.path.exists(wav_file):\n",
    "            print(f\"文件 {wav_file} 不存在\")\n",
    "            return\n",
    "    \n",
    "    print(f\"找到以下文件: \")\n",
    "    for wav_file in wav_files:\n",
    "        print(wav_file)\n",
    "    \n",
    "    # 加载5秒静音\n",
    "    combined = AudioSegment.silent(duration=5000)\n",
    "    \n",
    "    # 依次添加其他音频文件\n",
    "    for i, wav_file in enumerate(wav_files):\n",
    "        # 创建静音片段\n",
    "        pause = AudioSegment.silent(duration=pause_durations[i])\n",
    "        \n",
    "        # 加载并添加下一个音频文件\n",
    "        next_segment = AudioSegment.from_wav(wav_file)\n",
    "        combined = combined.append(next_segment, crossfade=crossfade_duration)\n",
    "\n",
    "        # 添加停顿\n",
    "        combined = combined + pause\n",
    "    \n",
    "    # 添加额外的静音以延长背景音乐\n",
    "    combined = combined + AudioSegment.silent(duration=music_extension_duration)\n",
    "    \n",
    "    # 加载背景音乐\n",
    "    background_music = AudioSegment.from_file(background_music_path)\n",
    "    \n",
    "    # 调整背景音乐音量\n",
    "    background_music = background_music - 15  # 减少音量，单位为dB\n",
    "    \n",
    "    # 截断或循环背景音乐以匹配合并音频的长度\n",
    "    if len(background_music) < len(combined):\n",
    "        background_music = background_music * ((len(combined) // len(background_music)) + 1)\n",
    "    background_music = background_music[:len(combined)]\n",
    "    \n",
    "    # 添加渐入和渐出效果\n",
    "    background_music = background_music.fade_in(fade_in_duration).fade_out(fade_out_duration)\n",
    "    \n",
    "    # 叠加背景音乐\n",
    "    combined_with_music = combined.overlay(background_music)\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # 导出合并后的文件\n",
    "    combined_with_music.export(output_path, format=\"wav\")\n",
    "    print(f\"已合并所有音频并添加背景音乐到：{output_path}\")\n",
    "\n",
    "# 使用示例\n",
    "base_path = \"output/tts/sleep/female2\"\n",
    "output_path = \"output/tts/sleep/female2_combined_with_bmusic_03.wav\"\n",
    "pause_durations = love_texts_silence_durations\n",
    "background_music_path = \"/root/autodl-tmp/I-AM/project/backend/agents/prompts/background_music/bmusic_03.wav\"\n",
    "music_extension_duration = 30000  # 背景音乐延长30秒\n",
    "fade_in_duration = 3000\n",
    "fade_out_duration = 10000  # 渐出时长10秒\n",
    "\n",
    "combine_wav_files_with_pause_and_music(\n",
    "    base_path,\n",
    "    output_path,\n",
    "    pause_durations,\n",
    "    background_music_path,\n",
    "    music_extension_duration,\n",
    "    fade_out_duration=fade_out_duration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manifest_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
