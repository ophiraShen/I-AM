{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 17:33:54,085 - modelscope - INFO - PyTorch version 2.5.1 Found.\n",
      "2025-01-10 17:33:54,090 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2025-01-10 17:33:54,126 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 2f5eeacd95c207bb2bc0f708cda0b4fd and a total number of 980 components indexed\n",
      "/root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to import ttsfrd, use WeTextProcessing instead\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/root/autodl-tmp/I-AM/CosyVoice\")\n",
    "sys.path.append(\"/root/autodl-tmp/I-AM/CosyVoice/third_party/Matcha-TTS\")\n",
    "\n",
    "import time\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import torchaudio, torch\n",
    "from contextlib import nullcontext\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from cosyvoice.cli.cosyvoice import CosyVoice, CosyVoice2\n",
    "from cosyvoice.utils.file_utils import load_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosyVoice2TTS:\n",
    "    def __init__(self, model_path, prompts_config_path):\n",
    "        self.cosyvoice = CosyVoice2(\n",
    "            model_path,\n",
    "            load_jit=True,\n",
    "            load_onnx=False,\n",
    "            load_trt=False\n",
    "        )\n",
    "        with open(prompts_config_path, 'r', encoding='utf-8') as file:\n",
    "            self.prompts_config = yaml.safe_load(file)\n",
    "\n",
    "    def generate_audio(\n",
    "        self,\n",
    "        texts_path,\n",
    "        voice_type,\n",
    "        background_music_type,\n",
    "        output_path,\n",
    "        speed=1.0,\n",
    "        stream=False,\n",
    "        music_volume=0.2,\n",
    "        music_extension_duration=30,\n",
    "        fade_in_duration=3,\n",
    "        fade_out_duration=10,\n",
    "        max_retries=3\n",
    "    ):\n",
    "\n",
    "        with open(texts_path, 'r', encoding='utf-8') as file:\n",
    "            text_segments = yaml.safe_load(file)['sequences']\n",
    "\n",
    "        prompt_speech_16k = load_wav(self.prompts_config['prompts']['speech'][voice_type], 16000)\n",
    "        prompt_text = self.prompts_config['prompts']['text'][voice_type]\n",
    "\n",
    "        total_len = len(text_segments)\n",
    "\n",
    "        success = False\n",
    "        attempt = 0\n",
    "\n",
    "        while not success and attempt < max_retries:\n",
    "            attempt += 1\n",
    "            audio_segments = [None] * total_len\n",
    "            failed = False\n",
    "\n",
    "            print(f\"Attempt {attempt} of {max_retries}\")\n",
    "            # 创建进度条\n",
    "            pbar = tqdm(total=total_len, desc=\"Generating audio segments\")\n",
    "\n",
    "            for idx in range(total_len):\n",
    "                audio = self._generate_single(\n",
    "                    text_segments[idx]['text'],\n",
    "                    prompt_text,\n",
    "                    prompt_speech_16k\n",
    "                )\n",
    "                if audio is None:\n",
    "                    print(f\"\\nFailed to generate audio for segment {idx}, retrying entire sequence...\")\n",
    "                    failed = True\n",
    "                    pbar.close()\n",
    "                    break\n",
    "\n",
    "                audio_segments[idx] = audio\n",
    "                pbar.update(1)\n",
    "            if not failed:\n",
    "                success = True\n",
    "                pbar.close()\n",
    "\n",
    "            if not success and attempt == max_retries:\n",
    "                raise Exception(\"Failed to generate all audio segments after maximum retries\")\n",
    "\n",
    "        print(\"Combining audio segments with background music...\")\n",
    "\n",
    "        combined_audio = self._combined_audios(\n",
    "            text_segments,\n",
    "            audio_segments,\n",
    "            background_music_type,\n",
    "            music_extension_duration,\n",
    "            fade_in_duration,\n",
    "            fade_out_duration\n",
    "        )\n",
    "\n",
    "        print(f\"Saving final audio to {output_path}\")\n",
    "\n",
    "        torchaudio.save(\n",
    "            output_path,\n",
    "            combined_audio,\n",
    "            self.cosyvoice.sample_rate\n",
    "        )\n",
    "\n",
    "        print(\"Audio generation completed!\")\n",
    "\n",
    "    def _generate_single(self, text, prompt_text, prompt_speech_16k, speed=1.0, stream=False):\n",
    "        try:\n",
    "            with torch.inference_mode(), torch.amp.autocast('cuda'):\n",
    "                for i, output in enumerate(self.cosyvoice.inference_zero_shot(\n",
    "                    text,\n",
    "                    prompt_text,\n",
    "                    prompt_speech_16k,\n",
    "                    speed=speed,\n",
    "                    stream=stream\n",
    "                )):\n",
    "                    audio = output['tts_speech']\n",
    "                return audio\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating audio for text: {text}\")\n",
    "            print(str(e))\n",
    "            return None\n",
    "\n",
    "    def _combined_audios(\n",
    "        self,\n",
    "        text_segments,\n",
    "        audio_segments,\n",
    "        background_music_type,\n",
    "        music_extension_duration=30,\n",
    "        fade_in_duration=3,\n",
    "        fade_out_duration=10\n",
    "    ):\n",
    "\n",
    "        combined_audio = self._generate_silence(fade_in_duration)\n",
    "        for i, audio in enumerate(audio_segments):\n",
    "            silence = self._generate_silence(text_segments[i]['duration'])\n",
    "            combined_audio = torch.cat([combined_audio, audio, silence], dim=1)\n",
    "        \n",
    "        music_extension_samples = self._generate_silence(music_extension_duration)\n",
    "        combined_audio =  torch.cat([combined_audio, music_extension_samples], dim=1)\n",
    "\n",
    "        background_music, bg_sample_rate = torchaudio.load(self.prompts_config['background_music'][background_music_type])\n",
    "\n",
    "        if background_music.shape[0] > 1:\n",
    "            background_music = torch.mean(background_music, dim=0, keepdim=True)\n",
    "\n",
    "        if bg_sample_rate != self.cosyvoice.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(bg_sample_rate, self.cosyvoice.sample_rate)\n",
    "            background_music = resampler(background_music)\n",
    "\n",
    "        # 调整背景音乐的长度以匹配语音长度\n",
    "        target_length = combined_audio.shape[1]\n",
    "        if background_music.shape[1] > target_length:\n",
    "            # 如果背景音乐更长，截取需要的部分\n",
    "            background_music = background_music[:, :target_length]\n",
    "        elif background_music.shape[1] < target_length:\n",
    "            # 如果背景音乐更短，循环播放直到达到所需长度\n",
    "            num_repeats = (target_length + background_music.shape[1] - 1) // background_music.shape[1]\n",
    "            background_music = background_music.repeat(1, num_repeats)\n",
    "            background_music = background_music[:, :target_length]\n",
    "\n",
    "        fade_in_samples = fade_in_duration * self.cosyvoice.sample_rate\n",
    "        fade_out_samples = fade_out_duration * self.cosyvoice.sample_rate\n",
    "\n",
    "        fade_in_curve = self._create_fade_curve(fade_in_samples, fade_in_samples, fade_in=True)\n",
    "        fade_out_curve = self._create_fade_curve(fade_out_samples, fade_out_samples, fade_in=False)\n",
    "\n",
    "        background_music[0, :fade_in_samples] *= fade_in_curve\n",
    "        background_music[0, -fade_out_samples:] *= fade_out_curve\n",
    "\n",
    "        # 调整背景音乐的音量（这里设置为语音的20%音量）\n",
    "        background_volume = 0.2\n",
    "        background_music = background_music * background_volume\n",
    "\n",
    "        # 混合语音和背景音乐\n",
    "        final_audio = combined_audio + background_music\n",
    "\n",
    "        # 防止音频溢出（可选）\n",
    "        if torch.max(torch.abs(final_audio)) > 1:\n",
    "            final_audio = final_audio / torch.max(torch.abs(final_audio))\n",
    "\n",
    "        return final_audio\n",
    "        \n",
    "\n",
    "    def _generate_silence(self, silence_duration):\n",
    "        return torch.zeros(1, silence_duration * self.cosyvoice.sample_rate)\n",
    "\n",
    "\n",
    "    def _create_fade_curve(self, length, fade_length, fade_in=True):\n",
    "        if fade_in:\n",
    "            return torch.linspace(0, 1, fade_length)\n",
    "        else:\n",
    "            return torch.linspace(1, 0, fade_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\n",
      "  deprecate(\"LoRACompatibleLinear\", \"1.0.0\", deprecation_message)\n",
      "2025-01-10 17:34:04,388 INFO input frame rate=25\n",
      "/root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/root/autodl-tmp/I-AM/CosyVoice/cosyvoice/dataset/processor.py:24: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend('soundfile')\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[1;31m2025-01-10 17:34:05.949812702 [E:onnxruntime:Default, provider_bridge_ort.cc:1744 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "\u001b[m\n",
      "\u001b[0;93m2025-01-10 17:34:05.949831408 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:870 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.\u001b[m\n",
      "2025-01-10 17:34:06,570 WETEXT INFO found existing fst: /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/zh_tn_tagger.fst\n",
      "2025-01-10 17:34:06,570 INFO found existing fst: /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/zh_tn_tagger.fst\n",
      "2025-01-10 17:34:06,572 WETEXT INFO                     /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/zh_tn_verbalizer.fst\n",
      "2025-01-10 17:34:06,572 INFO                     /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/zh_tn_verbalizer.fst\n",
      "2025-01-10 17:34:06,573 WETEXT INFO skip building fst for zh_normalizer ...\n",
      "2025-01-10 17:34:06,573 INFO skip building fst for zh_normalizer ...\n",
      "2025-01-10 17:34:06,959 WETEXT INFO found existing fst: /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/en_tn_tagger.fst\n",
      "2025-01-10 17:34:06,959 INFO found existing fst: /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/en_tn_tagger.fst\n",
      "2025-01-10 17:34:06,962 WETEXT INFO                     /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/en_tn_verbalizer.fst\n",
      "2025-01-10 17:34:06,962 INFO                     /root/miniconda3/envs/manifest_app/lib/python3.11/site-packages/tn/en_tn_verbalizer.fst\n",
      "2025-01-10 17:34:06,963 WETEXT INFO skip building fst for en_normalizer ...\n",
      "2025-01-10 17:34:06,963 INFO skip building fst for en_normalizer ...\n",
      "/root/autodl-tmp/I-AM/CosyVoice/cosyvoice/cli/model.py:291: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.llm.load_state_dict(torch.load(llm_model, map_location=self.device), strict=True)\n",
      "/root/autodl-tmp/I-AM/CosyVoice/cosyvoice/cli/model.py:293: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.flow.load_state_dict(torch.load(flow_model, map_location=self.device), strict=True)\n",
      "/root/autodl-tmp/I-AM/CosyVoice/cosyvoice/cli/model.py:297: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hift_state_dict = {k.replace('generator.', ''): v for k, v in torch.load(hift_model, map_location=self.device).items()}\n"
     ]
    }
   ],
   "source": [
    "tts = CosyVoice2TTS(\n",
    "    model_path='/root/autodl-fs/cosyvoice/pretrained_models/CosyVoice2-0.5B',\n",
    "    prompts_config_path='/root/autodl-tmp/I-AM/project/backend/agents/prompts/prompts_zero_shot.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating audio segments:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 17:34:14,047 INFO synthesis text 亲爱的朋友,请找一个舒适的地方躺下或坐下,闭上眼睛[breath],开始我们的宁静之旅。\n",
      "2025-01-10 17:34:22,677 INFO yield speech len 11.6, rtf 0.7439236599823524\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.52s/it]\n",
      "Generating audio segments:   8%|▊         | 1/12 [00:09<01:45,  9.55s/it]2025-01-10 17:34:23,590 INFO synthesis text 首先,深深地,吸一口气[breath],感受空气缓缓进入你的鼻腔,充满你的肺部。\n",
      "2025-01-10 17:34:33,141 INFO yield speech len 10.68, rtf 0.894291771485118\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.43s/it]\n",
      "Generating audio segments:  17%|█▋        | 2/12 [00:20<01:40, 10.09s/it]2025-01-10 17:34:34,059 INFO synthesis text 然后,慢慢地,呼出[breath],感受所有的紧张和压力[quick_breath]随着呼气离开你的身体。\n",
      "2025-01-10 17:34:43,120 INFO yield speech len 9.12, rtf 0.9935234983762106\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.95s/it]\n",
      "Generating audio segments:  25%|██▌       | 3/12 [00:29<01:30, 10.04s/it]2025-01-10 17:34:44,100 INFO synthesis text 再来一次,深深地,吸气[breath],慢慢地,呼气[breath]。让每一次呼吸[quick_breath]都带你进入更深层的放松状态。\n",
      "2025-01-10 17:34:57,242 INFO yield speech len 14.32, rtf 0.9177088570994372\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.08s/it]\n",
      "Generating audio segments:  33%|███▎      | 4/12 [00:44<01:33, 11.65s/it]2025-01-10 17:34:58,342 INFO synthesis text 现在,让我们从脚趾开始,慢慢地[breath]扫描你的身体。感受你的脚趾,它们是否感到沉重,或轻松。让它们<strong>完全放松</strong>。\n",
      "2025-01-10 17:35:10,098 INFO yield speech len 17.48, rtf 0.6725490638811473\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.83s/it]\n",
      "Generating audio segments:  42%|████▏     | 5/12 [00:56<01:24, 12.09s/it]2025-01-10 17:35:10,962 INFO synthesis text 接着,是你的脚踝[breath]、小腿[breath]、膝盖[breath],让每一块肌肉都放松下来。\n",
      "2025-01-10 17:35:17,637 INFO yield speech len 9.88, rtf 0.6756939386066637\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.51s/it]\n",
      "Generating audio segments:  50%|█████     | 6/12 [01:04<01:03, 10.54s/it]2025-01-10 17:35:18,675 INFO synthesis text 继续向上,感受你的大腿[breath]、臀部[breath]、腹部[breath],让它们都沉浸在舒适和放松中。\n",
      "2025-01-10 17:35:25,436 INFO yield speech len 10.12, rtf 0.668161330015763\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.78s/it]\n",
      "Generating audio segments:  58%|█████▊    | 7/12 [01:12<00:48,  9.64s/it]2025-01-10 17:35:26,324 INFO synthesis text 然后是胸部[breath]、肩膀[breath]、手臂[breath],直到你的指尖。\n",
      "2025-01-10 17:35:32,035 INFO yield speech len 8.2, rtf 0.6965609003857869\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.58s/it]\n",
      "Generating audio segments:  67%|██████▋   | 8/12 [01:18<00:34,  8.67s/it]2025-01-10 17:35:32,922 INFO synthesis text 最后,感受你的脖子[breath]、脸部[breath],让所有的紧张都消失。\n",
      "2025-01-10 17:35:38,823 INFO yield speech len 7.76, rtf 0.7605099186454852\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.77s/it]\n",
      "Generating audio segments:  75%|███████▌  | 9/12 [01:25<00:24,  8.08s/it]2025-01-10 17:35:39,687 INFO synthesis text 现在,想象你正躺在一片柔软的草地上,周围是宁静的夜晚。\n",
      "2025-01-10 17:35:45,023 INFO yield speech len 7.88, rtf 0.6770909437673346\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
      "Generating audio segments:  83%|████████▎ | 10/12 [01:31<00:15,  7.50s/it]2025-01-10 17:35:46,012 INFO synthesis text 你可以听到,微风轻轻拂过树叶的声音。感受到草地上的露珠,轻轻触碰你的皮肤。\n",
      "2025-01-10 17:35:54,417 INFO yield speech len 11.64, rtf 0.7220338914812225\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.38s/it]\n",
      "Generating audio segments:  92%|█████████▏| 11/12 [01:41<00:08,  8.08s/it]2025-01-10 17:35:55,439 INFO synthesis text 抬头看,夜空中繁星点点,月亮温柔地照耀着你。感受这份宁静和安详。让它们充满你的整个身体。\n",
      "2025-01-10 17:36:04,616 INFO yield speech len 14.64, rtf 0.6267877550073009\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.17s/it]\n",
      "Generating audio segments: 100%|██████████| 12/12 [01:51<00:00,  9.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining audio segments with background music...\n",
      "Saving final audio to /root/autodl-tmp/I-AM/project/backend/agents/jupyter/output/temp/sleep.wav\n",
      "Audio generation completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tts.generate_audio(\n",
    "    texts_path=\"/root/autodl-tmp/I-AM/project/backend/agents/prompts/texts/sleep_texts.yaml\",\n",
    "    voice_type=\"male1\",\n",
    "    background_music_type=\"bmusic_01\",\n",
    "    output_path=\"/root/autodl-tmp/I-AM/project/backend/agents/jupyter/output/temp/sleep.wav\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manifest_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
