{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# 设置关键库的日志级别\n",
    "loggers_to_quiet = [\n",
    "    'httpx',           # HTTP 客户端库\n",
    "    'httpcore',        # HTTP 核心库\n",
    "    'urllib3',         # HTTP 客户端库\n",
    "    'requests',        # HTTP 客户端库\n",
    "    'openai',          # OpenAI API 库\n",
    "    'torch',           # PyTorch\n",
    "    'transformers',    # Hugging Face Transformers\n",
    "    'langchain',       # LangChain\n",
    "    'langchain_core',  # LangChain Core\n",
    "    'tqdm',           # 进度条库\n",
    "    'numba',          # Numba\n",
    "    'matplotlib',      # Matplotlib\n",
    "]\n",
    "\n",
    "for logger_name in loggers_to_quiet:\n",
    "    logging.getLogger(logger_name).setLevel(logging.WARNING)\n",
    "\n",
    "# 如果还有调试信息，可以设置根日志记录器的级别\n",
    "logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/root/autodl-tmp/I-AM/project/backend/agents')\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "from langgraph.types import interrupt, Command\n",
    "from typing import TypedDict, Annotated, Literal, NotRequired, List, Optional, Any\n",
    "from pydantic import BaseModel, Field, ConfigDict, field_validator\n",
    "\n",
    "\n",
    "import uuid\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_log(current_log, new_log: str) -> list[str]:\n",
    "    if current_log is None:\n",
    "        return [new_log] if isinstance(new_log, str) else new_log\n",
    "    elif isinstance(current_log, list):\n",
    "        return current_log + [new_log] if isinstance(new_log, str) else new_log\n",
    "    elif isinstance(current_log, str):\n",
    "        return [current_log, new_log] if isinstance(new_log, str) else new_log\n",
    "    else:\n",
    "        return [new_log] if isinstance(new_log, str) else new_log\n",
    "\n",
    "class OverallState(BaseModel):\n",
    "    messages: Annotated[List[AnyMessage], add_messages] = Field(default_factory=list, title=\"对话列表\")\n",
    "    route: Literal[\"affirmation\", \"meditation\", \"normal_chat\"] = Field(default=\"normal_chat\", title=\"当前路由\")\n",
    "    log: Annotated[List[str], add_log] = Field(default_factory=list, title=\"日志列表\")\n",
    "\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    @field_validator('log', mode='before')\n",
    "    def validate_log(cls, v, info):\n",
    "        if v is None or (isinstance(v, list) and len(v) == 0):\n",
    "            return []\n",
    "        if 'log' in info.data:\n",
    "            return add_log(info.data['log'], v)\n",
    "        return [v] if isinstance(v ,str) else v\n",
    "        \n",
    "\n",
    "    @field_validator('messages', mode='before')\n",
    "    def validate_messages(cls, v, info):\n",
    "        if 'messages' in info.data:\n",
    "            return add_messages(info.data['messages'], v)\n",
    "        else:\n",
    "            return v if isinstance(v, list) else [v]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm(model_type: str = \"qwen2.5\", **kwargs):\n",
    "    \"\"\"直接获取 LLM 模型实例\"\"\"\n",
    "    \n",
    "    # 默认参数\n",
    "    default_params = {\n",
    "        \"temperature\": 0.9,\n",
    "        \"max_tokens\": 4096\n",
    "    }\n",
    "    # 合并用户传入的参数\n",
    "    params = {**default_params, **kwargs}\n",
    "    \n",
    "    if model_type == \"deepseek\":\n",
    "        return ChatOpenAI(\n",
    "            model=\"deepseek-chat\",\n",
    "            openai_api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "            openai_api_base='https://api.deepseek.com',\n",
    "            **params\n",
    "        )\n",
    "    elif model_type == \"qwen2.5\":\n",
    "        return ChatOpenAI(\n",
    "            model=\"qwen2.5\",\n",
    "            api_key=\"EMPTY\",\n",
    "            base_url=os.getenv(\"QWEN2.5_API_BASE\"),\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"不支持的模型类型: {model_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 肯定语模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from affirmation_agent import AffirmationAgent\n",
    "\n",
    "config_path = '/root/autodl-tmp/I-AM/project/backend/config/affirmation.yaml'\n",
    "\n",
    "affirmation_agent = AffirmationAgent().create_graph()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 冥想模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/root/autodl-tmp/I-AM/project/backend')\n",
    "# def create_meditation(state: OverallState) -> OverallState:\n",
    "#     return {\"route\": \"meditation\", \"log\": \"冥想音频已生成\"}\n",
    "\n",
    "# meditation_workflow = StateGraph(OverallState)\n",
    "# meditation_workflow.add_node(\"create_meditation\", create_meditation)\n",
    "# meditation_workflow.add_edge(START, \"create_meditation\")\n",
    "# meditation_workflow.add_edge(\"create_meditation\", END)\n",
    "\n",
    "# meditation_agent = meditation_workflow.compile()\n",
    "\n",
    "from agents.meditation_agent import MeditationAgent\n",
    "meditation_agent = MeditationAgent().create_graph()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用户反馈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_feadback_agent(state: OverallState) -> Command[Literal[\"affirmation_agent\", \"meditation_agent\", \"normal_chat_agent\"]]:\n",
    "    print(\"---user_feadback___\")\n",
    "    if state.route == \"affirmation\":\n",
    "        user_approval = interrupt(\n",
    "            {\n",
    "                \"question\": \"是否打开肯定语？\",\n",
    "                \"options\": [\"yes\", \"no\"],\n",
    "                \"default\": \"no\",\n",
    "            }\n",
    "        )\n",
    "        if user_approval == \"yes\":\n",
    "            return Command(goto=\"affirmation_agent\")\n",
    "        else:\n",
    "            return Command(goto=\"normal_chat_agent\")\n",
    "    elif state.route == \"meditation\":\n",
    "        user_approval = interrupt(\n",
    "            {\n",
    "                \"question\": \"是否打开冥想音频？\",\n",
    "                \"options\": [\"yes\", \"no\"],\n",
    "                \"default\": \"no\",\n",
    "            }\n",
    "        )\n",
    "        if user_approval == \"yes\":\n",
    "            return Command(goto=\"meditation_agent\")\n",
    "        else:\n",
    "            return Command(goto=\"normal_chat_agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 路由"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_deepseek = get_llm(model_type=\"deepseek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router(TypedDict):\n",
    "    route: Literal[\"affirmation\", \"meditation\", \"normal_chat\"]\n",
    "\n",
    "def router_node(state: OverallState) -> Command[Literal[\"user_feadback_agent\", \"normal_chat_agent\"]]:\n",
    "    print(\"---routing---\")\n",
    "    with open('/root/autodl-tmp/I-AM/project/backend/agents/prompts/chat/router_prompt.txt', 'r') as file:\n",
    "        router_prompt = file.read().strip()\n",
    "    router_prompt = ChatPromptTemplate([\n",
    "        (\"system\", router_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ])\n",
    "    router_model = router_prompt | llm_deepseek.with_structured_output(Router, method=\"function_calling\")\n",
    "    route_result = router_model.invoke({\"messages\": state.messages})\n",
    "\n",
    "    if route_result['route'] == \"affirmation\" or route_result['route'] == \"meditation\":\n",
    "        goto = \"user_feadback_agent\"\n",
    "    elif route_result['route'] == \"normal_chat\":\n",
    "        goto = \"normal_chat_agent\"\n",
    "    else:\n",
    "        goto = \"normal_chat_agent\"\n",
    "\n",
    "    return Command(\n",
    "        goto=goto,\n",
    "        update={\n",
    "            \"route\": route_result['route'],\n",
    "            \"log\": \"路由已确定\"\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主对话模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_chat_agent(state: OverallState) -> OverallState:\n",
    "    print(\"---normal_chat---\")\n",
    "    with open('/root/autodl-tmp/I-AM/project/backend/agents/prompts/chat/chat_prompt.txt', 'r') as file:\n",
    "        chat_prompt = file.read().strip()\n",
    "    normal_system_prompt = ChatPromptTemplate([\n",
    "        (\"system\", chat_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ])\n",
    "    prompt = normal_system_prompt.partial(route=state.route, log=\"\\n\".join(state.log))\n",
    "    normal_llm = prompt | llm_deepseek\n",
    "    response = normal_llm.invoke({\"messages\": state.messages})\n",
    "    return {\"route\": \"normal_chat\", \"messages\": [response], \"log\": \"常规对话已生成\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialogue_workflow = StateGraph(OverallState)\n",
    "main_dialogue_workflow.add_node(\"router_node\", router_node)\n",
    "main_dialogue_workflow.add_node(\"user_feadback_agent\", user_feadback_agent)\n",
    "main_dialogue_workflow.add_node(\"normal_chat_agent\", normal_chat_agent)\n",
    "main_dialogue_workflow.add_node(\"affirmation_agent\", affirmation_agent)\n",
    "main_dialogue_workflow.add_node(\"meditation_agent\", meditation_agent)\n",
    "\n",
    "main_dialogue_workflow.add_edge(\"affirmation_agent\", \"normal_chat_agent\")\n",
    "main_dialogue_workflow.add_edge(\"meditation_agent\", \"normal_chat_agent\")\n",
    "main_dialogue_workflow.add_edge(START, \"router_node\")\n",
    "main_dialogue_workflow.add_edge(\"normal_chat_agent\", END)\n",
    "\n",
    "\n",
    "memory = MemorySaver()\n",
    "main_dialogue_graph = main_dialogue_workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Setting xray to 1 will show the internal structure of the nested graph\n",
    "display(Image(main_dialogue_graph.get_graph(xray=2).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid4())}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"请为我生成一段面试的冥想音频\")]}\n",
    "for chunk in main_dialogue_graph.stream(inputs, config=config, stream_mode=\"updates\"):\n",
    "    # chunk['messages'][-1].pretty_print()\n",
    "    print(chunk)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in main_dialogue_graph.stream(Command(resume=\"yes\"), config=config, stream_mode='updates', subgraphs=True):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "\n",
    "# # 列出所有当前的 loggers\n",
    "# for name, logger in logging.root.manager.loggerDict.items():\n",
    "#     print(f\"Logger name: {name}\")\n",
    "#     if isinstance(logger, logging.Logger):\n",
    "#         print(f\"  Level: {logger.level}\")\n",
    "#         print(f\"  Handlers: {logger.handlers}\")\n",
    "#         print(f\"  Parent: {logger.parent}\")\n",
    "#     print(\"---\")\n",
    "\n",
    "# # 然后根据发现的 logger 名称设置级别\n",
    "# # 例如，如果发现是 'api_client' 在输出这些日志：\n",
    "# # logging.getLogger('api_client').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对话存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversation_json = {\n",
    "#     \"messages\": []\n",
    "# }\n",
    "\n",
    "# for d in chunk['messages']:\n",
    "#     if type(d).__name__ == 'HumanMessage':\n",
    "#         item = {\"role\": \"user\", \"content\": d.content}\n",
    "#     elif type(d).__name__ == 'AIMessage':\n",
    "#         item = {\"role\": \"assistant\", \"content\": d.content}\n",
    "#     else:\n",
    "#         continue\n",
    "#     conversation_json['messages'].append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('conversations_data/meditation.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(conversation_json, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manifest_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
